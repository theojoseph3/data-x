{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Notebook: NLP & NLTK\n",
    "#### Using Natural Language Processing in Python to do sentiment analysis on IMDB movie reviews\n",
    "\n",
    "Author: Alexander Fred Ojala\n",
    "\n",
    "Source: https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "https://github.com/rasbt/python-machine-learning-book/tree/master/code\n",
    "https://github.com/justmarkham/pycon-2016-tutorial\n",
    "\n",
    "*Code snippets given run on both Python 2.7 and Python 3.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 0: Pre-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division, absolute_import #make compatible with Python 2 and Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data description\n",
    "\n",
    "You can download the data (labeledTrainData.tsv.zip) here: https://www.kaggle.com/c/word2vec-nlp-tutorial/data, place it in your working directory & unzip the file.\n",
    "\n",
    "## Data set\n",
    "\n",
    "The labeled data set consists of 25,000 IMDB movie reviews. The sentiment of reviews is binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Part 1: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data to a Pandas data frame\n",
    "# Use header = 0 (first line contains col names)\n",
    "# use delimiter=\\t (columns are separated by tabs),\n",
    "# use quoting=3 (Python will ignore doubled quotes)\n",
    "\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Q1.1:\n",
    "* 1: How many movie reviews are positive and how many are negative in labeledTrainData.tsv?\n",
    "\n",
    "\n",
    "* 2: What is the average length of all the reviews (string length)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:1 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "\n",
      "1    12500\n",
      "0    12500\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/xdEuJdbLBu5gQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c591278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. \n",
    "print(train.shape)\n",
    "print()\n",
    "print(train.sentiment.value_counts())\n",
    "\n",
    "train.sentiment.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1329.71056\n"
     ]
    }
   ],
   "source": [
    "# 2.\n",
    "review_lengths = [len(review) for review in train.review.values]\n",
    "avg_length = sum(review_lengths) / len(review_lengths)\n",
    "print(avg_length)\n",
    "\n",
    "# 1329.71056 Python 3\n",
    "# 1329.95384 Python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Q1:2 Explore NLP on one review\n",
    "\n",
    "In Q1:2 you should work on the third review, i.e. `train['review'][2]`\n",
    "\n",
    "First we would like to clean up the reviews. As you can see many interviews contain \\ characters in front of quotation symobols, \"`<br/>` tags, numbers, abbrevations etc.\n",
    "\n",
    "* 1: Remove all the HTML tags in the third review, by creating a beatifulsoup object and then using the `.text` method. Save results in variable `review3`\n",
    "\n",
    "\n",
    "* 2: Import NLTK's sent_tokenizer and count the number of sentences in review 3 (cleaned from HTML tags). To import sent_tokenizer use: `from nltk.tokenize import sent_tokenize`\n",
    "\n",
    "\n",
    "* 3: Remove all punctuation, numbers and special characters from the third review (cleaned from HTML tags). We can do this using Regular Expression - package `re`. Code given to you as we haven't covered this in class. Save results in variable `review3`\n",
    "\n",
    "\n",
    "* 4: Convert all the letters to lower case (save in variable `review3`). Then split the string so that every word is one element in a list (save the list in variable `review3_words`). Note: When we split the strings into words that process is called tokenization.\n",
    "\n",
    "\n",
    "* 5: Use NLTK's PorterStemmer (`from nltk.stem import PorterStemmer`). Create a new Porter stemmer (`stemmer = PorterStemmer()`) and run it on every word in `review3_words`, print the results as one string (don't overwrite the `review3_words` variable from 4). What does the PorterStemmer do?\n",
    "\n",
    "\n",
    "* 6: Now we want to Part Of Speech (POS)) tag the third movie review. We will use POS labeling also called grammatical tagging. To do this import `from nltk.tag import pos_tag`. When you use pos_tag on a word it returns a token-tag pair in the form of a tuple. In NLTK's Penn Treebank POS, the abbreviation (tag) for an adjective is JJ and NN for singular nouns. Count the number of singular nouns (NN) and adjectives (JJ) in `review3_words` using NLTK's pos_tag. A list of the Penn Treebank pos_tag's can be found here: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "\n",
    "* 7: An even more sophisticated operation than stemming using the PorterStemmer is called lemmatizing. Lemmatizing, in contrast to stemming, does not create non-existent words and converts words to their synonyms. In order to use lemmatizing we need to define the wordnet POS tag. A function that takes in a POS Penn Treebank tag and converts it to a wordnet tag and then lemmatizes words in a string has been given to you below. Please extend this code and print the lemmatized third movie review. Don't save the results in any variable.\n",
    "\n",
    "\n",
    "* 8: Lastly we will try to remove common words that don't carry much information. These are called stopwords. In English they could for example be 'am', 'are', 'and' etc. To import NLTK's list of stopwords you need to download the stopword corpora (`import nltk` and then `nltk.download()` if you don't have it). When that is done run `from nltk.corpus import stopwords` and create a variable for English stopwords with `eng_stopwords = stopwords.words('english')`. Use the list of English stopwords to remove all the stopwords from your list of words in the third movie review, i.e. `review3_words`. Print `review3_words` without stopwords, count the number of stopwords removed and print them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:2 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.<br /><br />The movie delivers the goods\n"
     ]
    }
   ],
   "source": [
    "review3 = train['review'][2] # the review used for initial analysis\n",
    "print(review3[:1150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The film starts with a manager (Nicholas Bell) giving welcome investors (Robert Carradine) to Primal Park . A secret project mutating a primal animal using fossilized DNA, like ¨Jurassik Park¨, and some scientists resurrect one of nature's most fearsome predators, the Sabretooth tiger or Smilodon . Scientific ambition turns deadly, however, and when the high voltage fence is opened the creature escape and begins savagely stalking its prey - the human visitors , tourists and scientific.Meanwhile some youngsters enter in the restricted area of the security center and are attacked by a pack of large pre-historical animals which are deadlier and bigger . In addition , a security agent (Stacy Haiduk) and her mate (Brian Wimmer) fight hardly against the carnivorous Smilodons. The Sabretooths, themselves , of course, are the real star stars and they are astounding terrifyingly though not convincing. The giant animals savagely are stalking its prey and the group run afoul and fight against one nature's most fearsome predators. Furthermore a third Sabretooth more dangerous and slow stalks its victims.The movie delivers the goods with lots o\n"
     ]
    }
   ],
   "source": [
    "review3 = bs.BeautifulSoup(train['review'][2]).text # removes HTML tags\n",
    "print(review3[:1150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(sent_tokenize(review3))) # number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The film starts with a manager  Nicholas Bell  giving welcome investors  Robert Carradine  to Primal Park   A secret project mutating a primal animal using fossilized DNA  like  Jurassik Park   and s\n"
     ]
    }
   ],
   "source": [
    "review3 = re.sub('[^a-zA-Z]',' ',review3)\n",
    "print(review3[:200]) # remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'film', 'starts', 'with', 'a', 'manager', 'nicholas', 'bell', 'giving', 'welcome']\n"
     ]
    }
   ],
   "source": [
    "review3 = review3.lower()\n",
    "review3_words = review3.split()\n",
    "print(review3_words[:10]) # tokenize and lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the film start with a manag nichola bell give welcom investor robert carradin to primal park a secret project mutat a primal anim use fossil dna like jurassik park and some scientist resurrect one of natur s most fearsom predat the sabretooth tiger or smilodon scientif ambit turn deadli howev and when the high voltag fenc is open the creatur escap and begin savag stalk it prey the human visitor tourist and scientif meanwhil some youngster enter in the restrict area of the secur center and are attack by a pack of larg pre histor anim which are deadlier and bigger in addit a secur agent staci haiduk and her mate brian wimmer fight hardli against the carnivor smilodon the sabretooth themselv of cours are the real star star and they are astound terrifyingli though not convinc the giant anim savag are stalk it prey and the group run afoul and fight against one natur s most fearsom predat furthermor a third sabretooth more danger and slow stalk it victim the movi deliv the good with lot of blood and gore as behead hair rais chill full of scare when the sabretooth appear with mediocr special effect the stori provid excit and stir entertain but it result to be quit bore the giant anim are major made by comput gener and seem total lousi middl perform though the player react appropri to becom food actor give vigor physic perform dodg the beast run bound and leap or dangl over wall and it pack a ridicul final deadli scene no for small kid by realist gori and violent attack scene other film about sabretooth or smilodon are the follow sabretooth by jame r hickox with vanessa angel david keith and john rhi davi and the much better bc by roland emmerich with with steven strait cliff curti and camilla bell thi motion pictur fill with bloodi moment is badli direct by georg miller and with no origin becaus take too mani element from previou film miller is an australian director usual work for televis tidal wave journey to the center of the earth and mani other and occasion for cinema the man from snowi river zeu and roxann robinson cruso rate below averag bottom of barrel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "ps_stems = []\n",
    "for w in review3_words:\n",
    "    ps_stems.append(ps.stem(w))\n",
    "\n",
    "print(' '.join(ps_stems))\n",
    "    \n",
    "# PorterStemmer tries treating similar words as the same (e.g. give, gives, given = give)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nouns: 86\n",
      "Number of adjectives: 47\n"
     ]
    }
   ],
   "source": [
    "token_tag = pos_tag(review3_words)\n",
    "\n",
    "# count nouns (NN) and adjectives (JJ)\n",
    "NN_count = 0\n",
    "JJ_count = 0\n",
    "\n",
    "for pair in token_tag:\n",
    "    tag = pair[1]\n",
    "    if tag == 'JJ':\n",
    "        JJ_count+=1\n",
    "    elif tag == 'NN':\n",
    "        NN_count+=1\n",
    "print('Number of nouns:', NN_count)\n",
    "print('Number of adjectives:', JJ_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the film start with a manager nicholas bell give welcome investor robert carradine to primal park a secret project mutate a primal animal use fossilized dna like jurassik park and some scientist resurrect one of nature s most fearsome predator the sabretooth tiger or smilodon scientific ambition turn deadly however and when the high voltage fence be open the creature escape and begin savagely stalk it prey the human visitor tourist and scientific meanwhile some youngster enter in the restricted area of the security center and be attack by a pack of large pre historical animal which be deadlier and big in addition a security agent stacy haiduk and her mate brian wimmer fight hardly against the carnivorous smilodons the sabretooths themselves of course be the real star star and they be astound terrifyingly though not convince the giant animal savagely be stalk it prey and the group run afoul and fight against one nature s most fearsome predator furthermore a third sabretooth more dangerous and slow stalk it victim the movie deliver the good with lot of blood and gore a behead hair raise chill full of scare when the sabretooths appear with mediocre special effect the story provide exciting and stirring entertainment but it result to be quite bore the giant animal be majority make by computer generator and seem totally lousy middle performance though the player react appropriately to become food actor give vigorously physical performance dodge the beast run bound and leap or dangle over wall and it pack a ridiculous final deadly scene no for small kid by realistic gory and violent attack scenes other film about sabretooths or smilodon be the follow sabretooth by james r hickox with vanessa angel david keith and john rhys davy and the much good bc by roland emmerich with with steven strait cliff curtis and camilla belle this motion picture fill with bloody moment be badly direct by george miller and with no originality because take too many element from previous film miller be an australian director usually work for television tidal wave journey to the center of the earth and many others and occasionally for cinema the man from snowy river zeus and roxanne robinson crusoe rating below average bottom of barrel\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "wnl_stems = []\n",
    "for pair in token_tag:\n",
    "    res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "    wnl_stems.append(res)\n",
    "\n",
    "print(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film starts manager nicholas bell giving welcome investors robert carradine primal park secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one nature fearsome predators sabretooth tiger smilodon scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific meanwhile youngsters enter restricted area security center attacked pack large pre historical animals deadlier bigger addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star stars astounding terrifyingly though convincing giant animals savagely stalking prey group run afoul fight one nature fearsome predators furthermore third sabretooth dangerous slow stalks victims movie delivers goods lots blood gore beheading hair raising chills full scares sabretooths appear mediocre special effects story provides exciting stirring entertainment results quite boring giant animals majority made computer generator seem totally lousy middling performances though players reacting appropriately becoming food actors give vigorously physical performances dodging beasts running bound leaps dangling walls packs ridiculous final deadly scene small kids realistic gory violent attack scenes films sabretooths smilodon following sabretooth james r hickox vanessa angel david keith john rhys davies much better bc roland emmerich steven strait cliff curtis camilla belle motion picture filled bloody moments badly directed george miller originality takes many elements previous films miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe rating average bottom barrel\n",
      "\n",
      "Stop words removed ['the', 'with', 'a', 'to', 'a', 'a', 'and', 'some', 'of', 's', 'most', 'the', 'or', 'and', 'when', 'the', 'is', 'the', 'and', 'its', 'the', 'and', 'some', 'in', 'the', 'of', 'the', 'and', 'are', 'by', 'a', 'of', 'which', 'are', 'and', 'in', 'a', 'and', 'her', 'against', 'the', 'the', 'themselves', 'of', 'are', 'the', 'and', 'they', 'are', 'not', 'the', 'are', 'its', 'and', 'the', 'and', 'against', 's', 'most', 'a', 'more', 'and', 'its', 'the', 'the', 'with', 'of', 'and', 'as', 'of', 'when', 'the', 'with', 'the', 'and', 'but', 'it', 'to', 'be', 'the', 'are', 'by', 'and', 'the', 'to', 'the', 'and', 'or', 'over', 'and', 'it', 'a', 'no', 'for', 'by', 'and', 'other', 'about', 'or', 'are', 'the', 'by', 'with', 'and', 'and', 'the', 'by', 'with', 'with', 'and', 'this', 'with', 'is', 'by', 'and', 'with', 'no', 'because', 'too', 'from', 'is', 'an', 'for', 'to', 'the', 'of', 'the', 'and', 'and', 'for', 'the', 'from', 'and', 'below', 'of']\n",
      "\n",
      "NUMBER OF STOPWORDS REMOVED: 135\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "review3_wo_stopwords = [w for w in review3_words if not w in stopwords.words(\"english\")]\n",
    "removed_stopwords = [w for w in review3_words if w in stopwords.words(\"english\")]\n",
    "\n",
    "print(' '.join(review3_wo_stopwords))\n",
    "print()\n",
    "print('Stop words removed', removed_stopwords)\n",
    "print()\n",
    "print('NUMBER OF STOPWORDS REMOVED:',len(removed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:3\n",
    "\n",
    "* 1: Create a function called `review_cleaner` that reads in a review and\n",
    "    - Removes HTML tags (using beautifulsoup)\n",
    "    - Removes non-letters (using regular expression)\n",
    "    - Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "    - Removes all the English stopwords from the list of movie review words\n",
    "    - Joins the words back into one string seperated by space\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**\n",
    "    \n",
    "    \n",
    "* 2: Create three lists: \n",
    "    - `review_clean_original`, `review_clean_ps` and `review_clean_wnl`. Where `review_clean_original` contains all the reviews from the train DataFrame, that have been cleaned by the function `review_cleaner` defined in 1.\n",
    "    - `review_clean_ps` applies the PorterStemmer to the reviews in `review_clean_original`. **Note:** NLTK version 3.2.2 crashes when trying to use the PorterStemming on the string 'oed' (known bug). Therefore, use an if statement to skip just that specific string/word.\n",
    "    - `review_clean_wnl` contains words that have been lemmatized using NLTK's WordNetLemmatizer on the words in the list `review_clean_original`.\n",
    "    \n",
    "**Note, problem 2: can take more than 10minutes to run on a laptop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:3 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1. \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_cleaner(review):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "    \n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    \n",
    "    #1.\n",
    "    review = bs.BeautifulSoup(review).text\n",
    "    \n",
    "    #2.\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "    \n",
    "    #3.\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    #4.\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "    #5.\n",
    "    review = ' '.join(review)\n",
    "\n",
    "    return(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "CPU times: user 27.1 s, sys: 4.26 s, total: 31.4 s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_reviews = len(train['review'])\n",
    "\n",
    "review_clean_original = []\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    review_clean_original.append(review_cleaner(train['review'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "CPU times: user 1min 31s, sys: 723 ms, total: 1min 32s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Porter stemmer\n",
    "\n",
    "review_clean_ps = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    ps_stems = []\n",
    "    for w in review_clean_original[i].split():\n",
    "        if w == 'oed':\n",
    "            continue\n",
    "        ps_stems.append(ps.stem(w))\n",
    "    \n",
    "    review_clean_ps.append(' '.join(ps_stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "CPU times: user 4min 25s, sys: 2.45 s, total: 4min 27s\n",
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Lemmatizer\n",
    "\n",
    "review_clean_wnl = []\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1)%500 == 0 ):\n",
    "        # print progress\n",
    "        print(\"Done with %d reviews\" %(i+1)) \n",
    "    \n",
    "    wnl_stems = []\n",
    "    token_tag = pos_tag(review_clean_original[i].split())\n",
    "    for pair in token_tag:\n",
    "        res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "        wnl_stems.append(res)\n",
    "\n",
    "    review_clean_wnl.append(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:4: Feature vectors and Bag of Words model\n",
    "\n",
    "### Explanation\n",
    "\n",
    "Derived from source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "We will now use scikit-learn to create numeric representations of the words in the reviews, using a method called Bag of Words. You can see this as learning a vocabulary from all the reviews and counting how many times a word appears in the reviews. For example, if we have two sentences:\n",
    "\n",
    "**Sentence 1:** \"cool students study cool data science\"\n",
    "\n",
    "**Sentence 2:** \"to know data science study data science\"\n",
    "\n",
    "The vocabulary of these two sentences can be summarized in a dictionary:\n",
    "\n",
    "{ cool, students, study, data, science, to, know }\n",
    "\n",
    "The bags of words count the number of times each word occur in a sentence. In Sentence 1, \"cool\" appears twice, and \"students\", \"study\", \"data\", and \"science\" appear once. The feature vector for Sentence 1 is:\n",
    "\n",
    "Sentence 1: { 2, 1, 1, 1, 1, 0, 0 }\n",
    "\n",
    "And for Sentence 2: { 0, 0, 1, 2, 2, 1, 1}\n",
    "\n",
    "### Applying this strategy to the IMDB movie reviews\n",
    "\n",
    "The movie review data contains a lot of words. To limit the analysis we use the 5000 most frequent words from the cleaned reviews. To extract the bag of words features we will use scitkit-learn.\n",
    "\n",
    "The training data will be created by the `CountVectorizer` function from scikit-learn, and the training array will have 25000 rows (one for each review) and 5000 features (one for each vocabulary word).\n",
    "\n",
    "CountVectorizer can automatically handle text cleaning, but here we specify \"None\", instead we did a step-by-step cleaning of the data in the earlier problems.\n",
    "\n",
    "#### Random Forest for review sentiment classification\n",
    "\n",
    "First split up the data set so that 80% are used as training samples (the first 20000 reviews and their sentiment) and 20% are used as validation samples (the last 5000 reviews and their sentiment). Use Random Forest to do numeric training on the features for the training samples from the Bag of Words and their respective sentiment labels for each review / feature vector. The number of trees is set to 50 as a default value.\n",
    "\n",
    "## Problem\n",
    "\n",
    "* 1: Run this analysis for the three cleaned review lists, i.e. `review_clean_original`, `review_clean_ps` and `review_clean_wnl`, by using the code below. Extend the function to print the **validation accuracy** by using `forest.predict(train_data_features[20000:,:])` and then comparing the resulting sentiment predictions with the ones stored in `train[\"sentiment\"][20000:]`. **Note:** The printed validation accuracy should show the percentage of correctly predicted sentiments for the validation set.\n",
    "\n",
    "\n",
    "* 2: Print the validation accuracy obtained for the three models. **Note:** Takes about 4mins to run.\n",
    "\n",
    "\n",
    "* 3: What data preprocessing strategy worked the best? Why do you think that is? (Feel free to change the number of features extracted in the bag of words model and the number of trees in the random forest model, to see how it effects your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q1:4 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  0.9999 \n",
      " The validation accuracy is:  0.8328\n",
      "\n",
      "Porter Stemmer\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8298\n",
      "\n",
      "Lemmatizing\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      "The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8242\n",
      "\n",
      "CPU times: user 2min 1s, sys: 3.47 s, total: 2min 4s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def predict_sentiment(X,y=train[\"sentiment\"]):\n",
    "    \n",
    "    \n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool.\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 5000) \n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    train_data_features = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "    \n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_data_features[0:20000,:], y[0:20000] )\n",
    "    \n",
    "    \n",
    "    valid_predictions = forest.predict(train_data_features[20000:,:])\n",
    "    train_predictions = forest.predict(train_data_features[0:20000,:])\n",
    "    train_acc = np.mean(train_predictions == y[0:20000])\n",
    "    valid_acc = np.mean(valid_predictions == y[20000:])\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    \n",
    "print('Original Reviews')\n",
    "forest1 = predict_sentiment(review_clean_original)\n",
    "print('Porter Stemmer')\n",
    "forest2 = predict_sentiment(review_clean_ps)\n",
    "print('Lemmatizing')\n",
    "forest3 = predict_sentiment(review_clean_wnl)\n",
    "\n",
    "\n",
    "# It  seems like Porter Stemmer and Lemmatizing does not effect the results as much as we thought\n",
    "# This is just what Sebastian Raschka points out in his book Python Machine Learning:\n",
    "\n",
    "'''\n",
    "The Porter stemming algorithm is probably the oldest and simplest\n",
    "stemming algorithm. Other popular stemming algorithms include the\n",
    "newer Snowball stemmer (Porter2 or \"English\" stemmer) or the Lancaster\n",
    "stemmer (Paice-Husk stemmer), which is faster but also more aggressive\n",
    "than the Porter stemmer. Those alternative stemming algorithms are also\n",
    "available through the NLTK package (http://www.nltk.org/api/\n",
    "nltk.stem.html).\n",
    "\n",
    "While stemming can create non-real words, such as thu, (from thus) as\n",
    "shown in the previous example, a technique called lemmatization aims to\n",
    "obtain the canonical (grammatically correct) forms of individual words—\n",
    "the so-called lemmas. However, lemmatization is computationally more\n",
    "diffcult and expensive compared to stemming and, in practice, it has\n",
    "been observed that stemming and lemmatization have little impact on the\n",
    "performance of text classifcation (Michal Toman, Roman Tesar, and Karel\n",
    "Jezek. Infuence of word normalization on text classifcation. Proceedings of\n",
    "InSciT, pages 354–358, 2006).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 1000 of 25000\n",
      "\n",
      "Review 2000 of 25000\n",
      "\n",
      "Review 3000 of 25000\n",
      "\n",
      "Review 4000 of 25000\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 6000 of 25000\n",
      "\n",
      "Review 7000 of 25000\n",
      "\n",
      "Review 8000 of 25000\n",
      "\n",
      "Review 9000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 11000 of 25000\n",
      "\n",
      "Review 12000 of 25000\n",
      "\n",
      "Review 13000 of 25000\n",
      "\n",
      "Review 14000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 16000 of 25000\n",
      "\n",
      "Review 17000 of 25000\n",
      "\n",
      "Review 18000 of 25000\n",
      "\n",
      "Review 19000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 21000 of 25000\n",
      "\n",
      "Review 22000 of 25000\n",
      "\n",
      "Review 23000 of 25000\n",
      "\n",
      "Review 24000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you want to submit to Kaggle\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
    "                   quoting=3 )\n",
    "\n",
    "# Verify that there are 25,000 rows and 2 columns\n",
    "print(test.shape)\n",
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
    "for i in range(0,num_reviews):\n",
    "    if( (i+1) % 1000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
    "    clean_review = review_cleaner( test[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# Copy the results to a pandas dataframe with an \"id\" column and\n",
    "# a \"sentiment\" column\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "# Use pandas to write the comma-separated output file\n",
    "output.to_csv( \"BoW_results.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Extra Credit (worth 1p)\n",
    "\n",
    "* **Question:** Preprocess the reviews in any way you find suitable and build your own ML model that can predict the sentiment of movie reviews. Credit will be given if you can obtain a prediction accuracy of over 90%, when predicting the sentiments of the validation set (the last 5000 reviews). Train your model on the first 20000 reviews (with their sentiment as the target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## See other solution notebook ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
